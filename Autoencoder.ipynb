{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoders\n",
    "\n",
    "After initial analysis and some classification attempts with pca and lda, we discovered that our feature set did not differentiate well between users. To improve on the features we manually engineered, we will use an autoencoder to automatically generate features. Then we will apply some machine learning algorithms and compare results to the other techniques to evaluate if our newly generated feature set presents an improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data\n",
    "\n",
    "As an autoencoder is a neural network, we need to process our data in a way that it can be fed to the network. Similar to what was already done in the lda analysis, we will segment the data into equal time intervals, but instead of calculating features, we will take the mean value of values from a specific sensor that falls into each time bin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0,
     1
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "generate_csv_data(cont_bins=20, segment_intervals=[2, 10, 30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction\n",
    "\n",
    "After the data is calculated and stored in csv files, we can begin building our autoencoder, train it and extract the encoder that we will later use to generate features from the new examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation\n",
    "\n",
    "Befor we do any actual work with autoencoders, we need to define which data we want and subsample it accordingly (time intervals, users) and potentially perform some preprocessing such as normalization if it provides a significant increase in accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4390, 201)\n",
      "(3583, 201)\n"
     ]
    }
   ],
   "source": [
    "experiment = 1\n",
    "interval = 2\n",
    "use_bins = True\n",
    "\n",
    "data = read_csv(\"jupyter/data/raw_data_experiment_{}_segment_{}_seconds.csv\".format(experiment, interval)).fillna(0)\n",
    "X_train, X_test = split_csv_data(data, use_bins=use_bins)\n",
    "\n",
    "def extend_bins(data):\n",
    "    \"\"\"\n",
    "    Extend list of lists into a list and transform \n",
    "    the string list into an actual list.\n",
    "    \"\"\"\n",
    "    final_data = []\n",
    "    for row in data:\n",
    "        data_row = []\n",
    "        for x in row:\n",
    "            if type(x) == str:\n",
    "                data_row += [float(y) for y in x[1:-1].split(\",\")]\n",
    "            else:\n",
    "                data_row.append(x)\n",
    "        final_data.append(data_row)\n",
    "    return final_data\n",
    "\n",
    "\n",
    "def split_csv_data(data, use_bins=True):\n",
    "    \"\"\"\n",
    "    Split the data to a training and a testing set with each \n",
    "    user having one seance in training and other in testing.\n",
    "    \"\"\"\n",
    "    users = get_users_data(data)\n",
    "    train_seances = [x[0] for x in users.values()]\n",
    "    test_seances = [x[1] for x in users.values()]\n",
    "    training_set = data[data[\"seance\"].isin(train_seances)]\n",
    "    testing_set = data[data[\"seance\"].isin(test_seances)]\n",
    "    if use_bins:\n",
    "        X_train = training_set.iloc[:, 19:].values\n",
    "        X_test = testing_set.iloc[:, 19:].values\n",
    "    else:\n",
    "        X_train = training_set.iloc[:, 3:20].values\n",
    "        X_test = testing_set.iloc[:, 3:20].values\n",
    "\n",
    "    return nan_to_num(array(extend_bins(X_train))), nan_to_num(array(extend_bins(X_test)))\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the autoencoder\n",
    "\n",
    "After we acquired the data, we can define the autoencoder layers, with the appropriate dimensions, according to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4390 samples, validate on 3583 samples\n",
      "Epoch 1/20\n",
      "4390/4390 [==============================] - 1s 133us/step - loss: 12.8562 - val_loss: 12.7348\n",
      "Epoch 2/20\n",
      "4390/4390 [==============================] - 0s 96us/step - loss: 12.7372 - val_loss: 12.7335\n",
      "Epoch 3/20\n",
      "4390/4390 [==============================] - 0s 93us/step - loss: 12.7371 - val_loss: 12.7335\n",
      "Epoch 4/20\n",
      "4390/4390 [==============================] - 0s 94us/step - loss: 12.7362 - val_loss: 12.6952\n",
      "Epoch 5/20\n",
      "4390/4390 [==============================] - 0s 95us/step - loss: 12.7004 - val_loss: 12.6946\n",
      "Epoch 6/20\n",
      "4390/4390 [==============================] - 0s 94us/step - loss: 12.7004 - val_loss: 12.6946\n",
      "Epoch 7/20\n",
      "4390/4390 [==============================] - 0s 94us/step - loss: 12.7004 - val_loss: 12.6946\n",
      "Epoch 8/20\n",
      "4390/4390 [==============================] - 0s 95us/step - loss: 12.7003 - val_loss: 12.6946\n",
      "Epoch 9/20\n",
      "4390/4390 [==============================] - 0s 96us/step - loss: 12.7003 - val_loss: 12.6946\n",
      "Epoch 10/20\n",
      "4390/4390 [==============================] - 0s 96us/step - loss: 12.7003 - val_loss: 12.6946\n",
      "Epoch 11/20\n",
      "4390/4390 [==============================] - 0s 94us/step - loss: 12.7003 - val_loss: 12.6946\n",
      "Epoch 12/20\n",
      "4390/4390 [==============================] - 0s 94us/step - loss: 12.7003 - val_loss: 12.6946\n",
      "Epoch 13/20\n",
      "4390/4390 [==============================] - 0s 92us/step - loss: 12.7003 - val_loss: 12.6946\n",
      "Epoch 14/20\n",
      "4390/4390 [==============================] - 0s 93us/step - loss: 12.7003 - val_loss: 12.6946\n",
      "Epoch 15/20\n",
      "4390/4390 [==============================] - 0s 94us/step - loss: 12.7003 - val_loss: 12.6946\n",
      "Epoch 16/20\n",
      "4390/4390 [==============================] - 0s 93us/step - loss: 12.7003 - val_loss: 12.6946\n",
      "Epoch 17/20\n",
      "4390/4390 [==============================] - 0s 94us/step - loss: 12.7003 - val_loss: 12.6946\n",
      "Epoch 18/20\n",
      "4390/4390 [==============================] - 0s 94us/step - loss: 12.7003 - val_loss: 12.6946\n",
      "Epoch 19/20\n",
      "4390/4390 [==============================] - 0s 90us/step - loss: 12.7003 - val_loss: 12.6946\n",
      "Epoch 20/20\n",
      "4390/4390 [==============================] - 0s 95us/step - loss: 12.6988 - val_loss: 12.6925\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f302c7cf320>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input_img = Input(shape=(784,))\n",
    "# encoded = Dense(128, activation='relu')(input_img)\n",
    "# encoded = Dense(64, activation='relu')(encoded)\n",
    "# encoded = Dense(32, activation='relu')(encoded)\n",
    "\n",
    "# decoded = Dense(64, activation='relu')(encoded)\n",
    "# decoded = Dense(128, activation='relu')(decoded)\n",
    "# decoded = Dense(784, activation='sigmoid')(decoded)\n",
    "\n",
    "\n",
    "\n",
    "# this is the size of our encoded representations\n",
    "encoding_dim = 32  # 32 floats -> compression of factor 6.14, assuming the input is 207 floats\n",
    "\n",
    "# this is our input placeholder\n",
    "input_data = Input(shape=(X_train.shape[1],))\n",
    "\n",
    "encoded = Dense(128, activation='relu')(input_data)\n",
    "encoded = Dense(64, activation='relu')(encoded)\n",
    "encoded = Dense(32, activation='relu')(encoded)\n",
    "\n",
    "decoded = Dense(64, activation='relu')(encoded)\n",
    "decoded = Dense(128, activation='relu')(decoded)\n",
    "decoded = Dense(201, activation='sigmoid')(decoded)\n",
    "\n",
    "# # this model maps an input to its reconstruction\n",
    "# autoencoder = Model(input_data, decoded)\n",
    "\n",
    "# # this model maps an input to its encoded representation\n",
    "# encoder = Model(input_data, encoded)\n",
    "\n",
    "# # create a placeholder for an encoded (32-dimensional) input\n",
    "# encoded_input = Input(shape=(encoding_dim,))\n",
    "# # retrieve the last layer of the autoencoder model\n",
    "# decoder_layer = autoencoder.layers[-1]\n",
    "# # create the decoder model\n",
    "# decoder = Model(encoded_input, decoder_layer(encoded_input))\n",
    "\n",
    "autoencoder.compile(optimizer='adadelta', loss='msle')\n",
    "autoencoder.fit(X_train, X_train,\n",
    "                epochs=20,\n",
    "                batch_size=128,\n",
    "                shuffle=True,\n",
    "                validation_data=(X_test, X_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.44545000e+04  2.22222222e-02  2.11111111e-02  2.00000000e-02\n",
      "   2.05555556e-02  2.00000000e-02  2.11111111e-02  2.11111111e-02\n",
      "   2.27777778e-02  2.11111111e-02  2.11111111e-02  2.05555556e-02\n",
      "   1.94444444e-02  1.94444444e-02  2.05555556e-02  2.11111111e-02\n",
      "   2.05555556e-02  2.16666667e-02  2.11111111e-02  2.27777778e-02\n",
      "   2.00000000e-02 -2.50000000e-02 -2.77777778e-02 -2.66666667e-02\n",
      "  -2.77777778e-02 -2.77777778e-02 -2.66666667e-02 -2.72222222e-02\n",
      "  -2.55555556e-02 -2.55555556e-02 -2.50000000e-02 -2.44444444e-02\n",
      "  -2.50000000e-02 -2.94444444e-02 -2.83333333e-02 -2.72222222e-02\n",
      "  -2.50000000e-02 -2.55555556e-02 -2.83333333e-02 -2.77777778e-02\n",
      "  -2.61111111e-02  8.78888889e-01  8.71666667e-01  8.77222222e-01\n",
      "   8.75555556e-01  8.78333333e-01  8.76111111e-01  8.75555556e-01\n",
      "   8.77777778e-01  8.77777778e-01  8.76111111e-01  8.75000000e-01\n",
      "   8.77777778e-01  8.80000000e-01  8.80000000e-01  8.75555556e-01\n",
      "   8.78333333e-01  8.77777778e-01  8.77222222e-01  8.78888889e-01\n",
      "   8.77777778e-01 -5.17222222e-01 -4.97777778e-01 -5.28333333e-01\n",
      "  -5.44444444e-01 -5.38333333e-01 -5.08888889e-01 -5.55000000e-01\n",
      "  -5.04444444e-01 -5.04444444e-01 -5.46666667e-01 -5.00000000e-01\n",
      "  -5.51666667e-01 -5.16666667e-01 -5.42222222e-01 -5.32777778e-01\n",
      "  -5.24444444e-01 -5.41666667e-01 -5.42222222e-01 -5.42777778e-01\n",
      "  -5.21111111e-01  1.81555556e+00  1.83944444e+00  1.75833333e+00\n",
      "   1.81611111e+00  1.83222222e+00  1.80777778e+00  1.79277778e+00\n",
      "   1.80833333e+00  1.79555556e+00  1.75777778e+00  1.83333333e+00\n",
      "   1.79111111e+00  1.79555556e+00  1.80555556e+00  1.82722222e+00\n",
      "   1.80555556e+00  1.81222222e+00  1.81611111e+00  1.79444444e+00\n",
      "   1.76000000e+00 -1.71444444e+00 -1.67500000e+00 -1.74555556e+00\n",
      "  -1.69888889e+00 -1.69277778e+00 -1.71722222e+00 -1.71722222e+00\n",
      "  -1.70000000e+00 -1.70666667e+00 -1.67333333e+00 -1.72000000e+00\n",
      "  -1.67777778e+00 -1.72222222e+00 -1.72055556e+00 -1.71333333e+00\n",
      "  -1.71500000e+00 -1.73833333e+00 -1.71166667e+00 -1.73166667e+00\n",
      "  -1.75388889e+00  7.00105263e+02  6.99842105e+02  6.99947368e+02\n",
      "   6.99947368e+02  7.00157895e+02  7.00105263e+02  7.00052632e+02\n",
      "   7.00157895e+02  7.00157895e+02  7.00263158e+02  7.00000000e+02\n",
      "   7.00052632e+02  6.99842105e+02  6.99947368e+02  7.00210526e+02\n",
      "   7.00000000e+02  7.00157895e+02  6.99947368e+02  7.00157895e+02\n",
      "   6.99842105e+02  5.58000000e+02  5.57950000e+02  5.58350000e+02\n",
      "   5.58000000e+02  5.58100000e+02  5.58100000e+02  5.58150000e+02\n",
      "   5.58050000e+02  5.58150000e+02  5.57900000e+02  5.58000000e+02\n",
      "   5.58200000e+02  5.58050000e+02  5.58050000e+02  5.58200000e+02\n",
      "   5.57900000e+02  5.58200000e+02  5.57950000e+02  5.58150000e+02\n",
      "   5.57900000e+02  6.20736842e+02  6.20631579e+02  6.20473684e+02\n",
      "   6.20526316e+02  6.20526316e+02  6.20473684e+02  6.20842105e+02\n",
      "   6.20684211e+02  6.20526316e+02  6.20684211e+02  6.20631579e+02\n",
      "   6.20315789e+02  6.20421053e+02  6.20684211e+02  6.20421053e+02\n",
      "   6.20631579e+02  6.20789474e+02  6.20947368e+02  6.20631579e+02\n",
      "   6.20842105e+02  2.00000000e-01  2.50000000e-01  1.50000000e-01\n",
      "   3.00000000e-01  2.00000000e-01  2.00000000e-01  2.50000000e-01\n",
      "   1.50000000e-01  3.00000000e-01  2.00000000e-01  2.50000000e-01\n",
      "   2.00000000e-01  2.50000000e-01  1.50000000e-01  4.00000000e-01\n",
      "   1.00000000e-01  1.00000000e-01  1.00000000e-01  5.00000000e-02\n",
      "   1.50000000e-01]]\n",
      "[[1662.9952      0.       1892.5734   2046.7755      0.          0.\n",
      "     0.       2354.4595   2368.6514      0.          0.          0.\n",
      "  1852.0953   1180.6902      0.       3568.9001      0.       3355.2756\n",
      "     0.       1704.7855      0.       3279.5098      0.       2177.7542\n",
      "     0.          0.       2408.879    3300.8735      0.       3128.4336\n",
      "    76.831024    0.      ]\n",
      " [1663.3098      0.       1893.4003   2047.6335      0.          0.\n",
      "     0.       2354.8005   2369.539       0.          0.          0.\n",
      "  1853.06     1181.5651      0.       3569.8105      0.       3356.3765\n",
      "     0.       1705.0219      0.       3280.4556      0.       2178.1768\n",
      "     0.          0.       2409.3625   3301.6917      0.       3129.3684\n",
      "    76.89094     0.      ]]\n"
     ]
    }
   ],
   "source": [
    "print(X_test[:1])\n",
    "print(encoder.predict(X_test[:2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers\n",
    "\n",
    "Functions that are used in this document, but moved here to reduce the clutter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "code_folding": [
     7,
     55
    ]
   },
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from numpy import mean, array, isnan, unique, nan_to_num\n",
    "from pandas import read_csv, DataFrame\n",
    "\n",
    "\n",
    "def get_users_data(data):\n",
    "    \"\"\"\n",
    "    Get seance id for each user in a form of a dict.\n",
    "    \"\"\"\n",
    "    users = {}\n",
    "    for user in list(set(data[\"user\"])):\n",
    "        x = data[data[\"user\"] == user]\n",
    "        users.update({user: sorted(list(set(x[\"seance\"])))})\n",
    "    return users\n",
    "\n",
    "\n",
    "def generate_csv_data(cont_bins=20, segment_intervals=[2, 10, 30, 60, 90, 120]):\n",
    "    sensors = {\n",
    "        \"ax\": 60,\n",
    "        \"ay\": 61,\n",
    "        \"az\": 62,\n",
    "        \"gx\": 63,\n",
    "        \"gy\": 64,\n",
    "        \"gz\": 65,\n",
    "        \"fa\": 77,\n",
    "        \"fb\": 76,\n",
    "        \"fc\": 54,\n",
    "        \"fd\": 55,\n",
    "        \"ca\": 78,\n",
    "        \"cb\": 79,\n",
    "        \"cc\": 80,\n",
    "        \"cd\": 81,\n",
    "        \"me\": 82,\n",
    "        \"nr\": 84,\n",
    "        \"ns\": 83,\n",
    "    }\n",
    "    for interval in segment_intervals:\n",
    "        # Experiments\n",
    "        for ex in [1, 2, 3]:\n",
    "            data = {\n",
    "                \"user\": [],\n",
    "                \"seance\": [],\n",
    "                \"time\": [],\n",
    "                \"ax\": [],\n",
    "                \"ay\": [],\n",
    "                \"az\": [],\n",
    "                \"gx\": [],\n",
    "                \"gy\": [],\n",
    "                \"gz\": [],\n",
    "                \"fa\": [],\n",
    "                \"fb\": [],\n",
    "                \"fc\": [],\n",
    "                \"fd\": [],\n",
    "                \"ca\": [],\n",
    "                \"cb\": [],\n",
    "                \"cc\": [],\n",
    "                \"cd\": [],\n",
    "                \"me\": [],\n",
    "                \"nr\": [],\n",
    "                \"ns\": [],\n",
    "                \"ax_b\": [],\n",
    "                \"ay_b\": [],\n",
    "                \"az_b\": [],\n",
    "                \"gx_b\": [],\n",
    "                \"gy_b\": [],\n",
    "                \"gz_b\": [],\n",
    "                \"fa_b\": [],\n",
    "                \"fb_b\": [],\n",
    "                \"fc_b\": [],\n",
    "                \"fd_b\": [],\n",
    "            }\n",
    "            seances = Seance.objects.filter(\n",
    "                experiment__sequence_number=ex, valid=True\n",
    "            ).order_by(\"created\")\n",
    "            seance_count = seances.count()\n",
    "            print(\"Processing {} seances with experiment {}\".format(seance_count, ex))\n",
    "            curr_seance = 1\n",
    "            for seance in seances:\n",
    "                print(\"{} of {}\".format(curr_seance, seance_count))\n",
    "                print(seance)\n",
    "                curr_seance += 1\n",
    "                start = seance.start\n",
    "\n",
    "                # Seconds from seance start\n",
    "                i = 0\n",
    "                # Iterate through seance\n",
    "                while start < seance.end:\n",
    "                    data[\"user\"].append(seance.user.id)\n",
    "                    data[\"seance\"].append(seance.id)\n",
    "                    data[\"time\"].append(i * interval)\n",
    "                    # Get records for all sensors\n",
    "                    records = SensorRecord.objects.filter(\n",
    "                        timestamp__range=(start, start + timedelta(seconds=interval)),\n",
    "                        seance=seance,\n",
    "                    ).order_by(\"timestamp\")\n",
    "                    # Calculate final data on per sensor basis\n",
    "                    for sensor in sensors:\n",
    "                        sensor_records = [\n",
    "                            x.value for x in records.filter(sensor__id=sensors[sensor])\n",
    "                        ]\n",
    "                        # Create multiple bins of data, if data from accelerometer, gyroscope or force sensor\n",
    "                        if sensor in [\n",
    "                            \"ax\",\n",
    "                            \"ay\",\n",
    "                            \"az\",\n",
    "                            \"gx\",\n",
    "                            \"gy\",\n",
    "                            \"gz\",\n",
    "                            \"fa\",\n",
    "                            \"fb\",\n",
    "                            \"fc\",\n",
    "                            \"fd\",\n",
    "                        ]:\n",
    "                            step = int(len(sensor_records) / cont_bins)\n",
    "                            bins = []\n",
    "                            for j in range(0, cont_bins):\n",
    "                                sub_records = sensor_records[j * step : (j + 1) * step]\n",
    "                                bins.append(mean(sub_records))\n",
    "                            data[sensor + \"_b\"].append(bins)\n",
    "                        if not sensor_records:\n",
    "                            data[sensor].append(0)\n",
    "                        else:\n",
    "                            data[sensor].append(mean(sensor_records))\n",
    "                    i += 1\n",
    "                    start += timedelta(seconds=interval)\n",
    "            df = DataFrame(data)\n",
    "            df.to_csv(\n",
    "                \"raw_data_experiment_{}_segment_{}_seconds.csv\".format(ex, interval),\n",
    "                index=False,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed seance started at: 2019-09-20 10:19:29 with user test_subject_04\n",
      "Completed seance started at: 2019-09-13 08:44:53 with user test_subject_01\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-d1204fa51e95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-d1204fa51e95>\u001b[0m in \u001b[0;36mprocess_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m                         \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                         for x in SensorRecord.objects.filter(\n\u001b[0;32m---> 26\u001b[0;31m                             \u001b[0mseance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msensor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m                         )\n\u001b[1;32m     28\u001b[0m                     ],\n",
      "\u001b[0;32m~/.virtualenvs/cabackend/lib/python3.7/site-packages/django/db/models/query.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    266\u001b[0m                \u001b[0;34m-\u001b[0m \u001b[0mResponsible\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mturning\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mrows\u001b[0m \u001b[0minto\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mobjects\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \"\"\"\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/cabackend/lib/python3.7/site-packages/django/db/models/query.py\u001b[0m in \u001b[0;36m_fetch_all\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1184\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fetch_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result_cache\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1186\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterable_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1187\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prefetch_related_lookups\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prefetch_done\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prefetch_related_objects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/cabackend/lib/python3.7/site-packages/django/db/models/query.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mrelated_populators\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_related_populators\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mklass_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcompiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_cls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_db\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_fields_start\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmodel_fields_end\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mrel_populator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrelated_populators\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0mrel_populator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/cabackend/lib/python3.7/site-packages/django/db/models/base.py\u001b[0m in \u001b[0;36mfrom_db\u001b[0;34m(cls, db, field_names, values)\u001b[0m\n\u001b[1;32m    495\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_meta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcrete_fields\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m             ]\n\u001b[0;32m--> 497\u001b[0;31m         \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    498\u001b[0m         \u001b[0mnew\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m         \u001b[0mnew\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/cabackend/lib/python3.7/site-packages/django/db/models/base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'%s' is an invalid keyword argument for this function\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mkwarg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m         \u001b[0mpost_init\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msender\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/cabackend/lib/python3.7/site-packages/django/dispatch/dispatcher.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, sender, **named)\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_live_receivers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msender\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msender\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mnamed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m         \"\"\"\n\u001b[1;32m    154\u001b[0m         \u001b[0mSend\u001b[0m \u001b[0msignal\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msender\u001b[0m \u001b[0mto\u001b[0m \u001b[0mall\u001b[0m \u001b[0mconnected\u001b[0m \u001b[0mreceivers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from numpy import mean, std\n",
    "\n",
    "from seances.models import Seance\n",
    "from sensors.models import SensorRecord, Sensor\n",
    "\n",
    "SENSORS = [\"fsr_01\",\"fsr_02\",\"fsr_03\",\"fsr_04\",\"accel01_x\",\"accel01_y\",\"accel01_z\",\"gyro01_x\",\"gyro01_y\",\"gyro01_z\",\"cpuusage_01\",\"cpuusage_02\",\"cpuusage_03\",\"cpuusage_04\",\"mempercentage_01\",\"netpacketssent_01\",\"netpacketsreceived_01\"]\n",
    "\n",
    "def process_data():\n",
    "    \"\"\"\n",
    "    Shape our data in a way that is usable by the autoencoder.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    seances = Seance.objects.filter(valid=True, experiment__sequence_number=1)\n",
    "    sensors = Sensor.objects.filter(topic__in=SENSORS)\n",
    "\n",
    "    for seance in seances:\n",
    "        print(seance)\n",
    "        row_data = {\"user_id\": seance.user.id, \"seance_id\": seance.id}\n",
    "        valid = True\n",
    "        for sensor in sensors:\n",
    "            try:\n",
    "                sensor_data = to_n_points(\n",
    "                    [\n",
    "                        x.value\n",
    "                        for x in SensorRecord.objects.filter(\n",
    "                            seance=seance, sensor=sensor\n",
    "                        )\n",
    "                    ],\n",
    "                    50,\n",
    "                )\n",
    "            except ValueError:\n",
    "                print(\"Missin data in seance... skipping.\")\n",
    "                valid = False\n",
    "                break\n",
    "            row_data.update({sensor.topic: sensor_data})\n",
    "        if valid:\n",
    "            data.append(row_data)\n",
    "    return data\n",
    "\n",
    "def to_n_points(data: list, n: int):\n",
    "    \"\"\"\n",
    "    Take the provided list of values and compress it to a length of n elements.\n",
    "    This is achieved by averaging elements.\n",
    "    \"\"\"\n",
    "    if len(data) < n:\n",
    "        raise ValueError(\"Not enough data to compress to {} elements.\".format(n))\n",
    "\n",
    "    step = len(data) / n\n",
    "    i = 0\n",
    "    result = []\n",
    "    for _ in range(0, n):\n",
    "        row = data[round(i) : round(i + step)]\n",
    "        result.append(mean(row))\n",
    "        i += step\n",
    "    return result\n",
    "\n",
    "data = process_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X = []\n",
    "for row in data:\n",
    "    x = []\n",
    "    for sensor in SENSORS:\n",
    "        x += row[sensor]\n",
    "    X.append(x)\n",
    "\n",
    "print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "# this is the size of our encoded representations\n",
    "encoding_dim = 16  # 32 floats -> compression of factor 24.5, assuming the input is 784 floats\n",
    "\n",
    "# this is our input placeholder\n",
    "input_img = Input(shape=(784,))\n",
    "# \"encoded\" is the encoded representation of the input\n",
    "encoded = Dense(encoding_dim, activation=\"relu\")(input_img)\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "decoded = Dense(784, activation=\"sigmoid\")(encoded)\n",
    "\n",
    "# this model maps an input to its reconstruction\n",
    "autoencoder = Model(input_img, decoded)\n",
    "\n",
    "# this model maps an input to its encoded representation\n",
    "encoder = Model(input_img, encoded)\n",
    "\n",
    "# create a placeholder for an encoded (32-dimensional) input\n",
    "encoded_input = Input(shape=(encoding_dim,))\n",
    "# retrieve the last layer of the autoencoder model\n",
    "decoder_layer = autoencoder.layers[-1]\n",
    "# create the decoder model\n",
    "decoder = Model(encoded_input, decoder_layer(encoded_input))\n",
    "\n",
    "autoencoder.compile(optimizer=\"adadelta\", loss=\"binary_crossentropy\")\n",
    "\n",
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "\n",
    "autoencoder.fit(\n",
    "    x_train,\n",
    "    x_train,\n",
    "    epochs=50,\n",
    "    batch_size=256,\n",
    "    shuffle=True,\n",
    "    validation_data=(x_test, x_test),\n",
    ")\n",
    "\n",
    "# encode and decode some digits\n",
    "# note that we take them from the *test* set\n",
    "encoded_imgs = encoder.predict(x_test)\n",
    "decoded_imgs = decoder.predict(encoded_imgs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Django Shell-Plus",
   "language": "python",
   "name": "django_extensions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "961.85px",
    "left": "1550px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
